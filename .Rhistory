library(datasets)
data(anscombe)
anscombe
# Sample size
n=dim(anscombe)[1]
n
# Compute means, variances, correlations
sample.means=apply(anscombe,2,mean)
sample.means
sample.var=apply(anscombe,2,var)
sample.var
cor=cor(anscombe[,1:4],anscombe[,5:8])
cor
### CASE 1 : predictor = x1, response = y1
X=anscombe$x1
Y=anscombe$y1
# Scatterplot of Y vs X
x11()
plot(X,Y,main='Scatterplot of Y vs X, case 1', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[1]), ylab=expression(y[1]))
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 1', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[1]), ylab=expression(y[1]))
# Linear regression
result1=lm(Y ~ X)
summary(result1)
# Add regression line to the plot
coef1=result1$coef
abline(coef1[1],coef1[2],col='red',lwd=2)
#or
abline(result1, col = "red", lwd = 2 )
### CASE 2 : predictor = x2, response = y2
X=anscombe$x2
Y=anscombe$y2
# Scatterplot of Y vs X
x11()
plot(X,Y,main='Scatterplot of Y vs X, case 2', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[2]), ylab=expression(y[2]))
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 2', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[2]), ylab=expression(y[2]))
# Linear regression
result2=lm(Y ~ X)
summary(result2)
# Add regression line to the plot
coef2=result2$coef
abline(coef2[1],coef2[2],col='red',lwd=2)
### CASE 3 : predictor = x3, response = y3
X=anscombe$x3
Y=anscombe$y3
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 3', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[3]), ylab=expression(y[3]))
# How to detect and remove the outlier?
which(Y > 10)
X[which(Y > 10)]
Y[which(Y > 10)]
X = X[-which(Y > 10)]
Y = Y[-which(Y > 10)]
# Linear regression
result3=lm(Y ~ X)
summary(result3)
# Add regression line to the plot
coef3=result3$coef
abline(coef3[1],coef3[2],col='red',lwd=2)
### CASE 4 : predictor = x4, response = y4
X=anscombe$x4
Y=anscombe$y4
# Scatterplot of Y vs X
x11()
plot(X,Y,main='Scatterplot of Y vs X, case 4', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[4]), ylab=expression(y[4]))
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 4', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[4]), ylab=expression(y[4]))
# Linear regression
result4=lm(Y ~ X)
summary(result4)
# Add regression line to the plot
coef4=result4$coef
abline(coef4[1],coef4[2],col='red',lwd=2)
# Get residuals
res1=result1$residuals
res2=result2$residuals
res3=result3$residuals
res4=result4$residuals
# Plot of residuals vs X, in the 4 cases
x11()
# Plot of residuals vs X, in the 4 cases
quartz()
par(mfrow=c(2,2))
plot(anscombe$x1,res1,main='Residuals vs X, case 1',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x2,res2,main='Residuals vs X, case 2',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x3,res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x4,res4,main='Residuals vs X, case 4',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
# Get fitted Y
Y_hat1=result1$fitted.values
Y_hat2=result2$fitted.values
Y_hat3=result3$fitted.values
Y_hat4=result4$fitted.values
# Plot of residuals vs fitted Y, in the 4 cases
quartz()
par(mfrow=c(2,2))
plot(Y_hat1,res1,main='Residuals vs fitted Y, case 1',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(Y_hat2,res2,main='Residuals vs fitted Y, case 2',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(Y_hat3,res3,main='Residuals vs fitted Y, case 3',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(Y_hat4,res4,main='Residuals vs fitted Y, case 4',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
##---------------------------------------------------------------
## LAB_1 - BLOCK 3
##---------------------------------------------------------------
# Topics:
# - Linear regression
# - Multiple linear regression
setwd('/Users/lucadanese/Library/CloudStorage/GoogleDrive-l.danese1@campus.unimib.it/My Drive/Tutoraggi/Politecnico/Statistics@Hunimed - 25:26/Block 3')
## LINEAR REGRESSION ----
# EXERCISE 1 --------------------------------------------------------------
data = read.table(file='AppendiceA.txt', header = T)
dim(data)
str(data)
pairs(data[,2:4],col = ifelse( data$Sex == 'F', 'pink', 'royalblue'))
# We want to see if there is a linear relationship between Weight
# (variable data$Weight, which we will call Y) and the Cholesterol
# (variable data$Cholesterol, which we will call X).
# let's graphically analyze the relationship between the two variables
plot( data$Cholesterol, data$Weight, pch = 16, xlab = 'Cholesterol', ylab = 'Weight' )
plot( data$Cholesterol, data$Weight, pch = 16, xlab = 'Cholesterol', ylab = 'Weight',
col = ifelse( data$Sex == 'F', 'pink', 'royalblue' ) )
legend( 'bottomright', fill = c( 'pink', 'royalblue' ), legend = c( 'Females', 'males' ) )
# We only want to study males, so we isolate them
# and build a new data.frame
males = data[ which(data$Sex == 'M' ), ]
# review the graph, considering only males
plot( males$Cholesterol, males$Weight, pch = 16, xlab = 'Cholesterol', ylab = 'Weight',
col = 'royalblue')
## SIMPLE REGRESSION (one regressor)
#Regression is a method of estimating the numerical relationship between variables
#We are interested in how well one variable can be used to predict another
#We have 2 kinds of variables:
#the outcome variable which we are trying to predict: dependent variable
#the predictor or explanatory variable (they could be more than one): independent variable(s)
### Linear regression Y=beta_0+beta_1*X+epsilon
###                   epsilon: random error with mean 0 and variance sigma^2
### E[Y|X] = beta_0+beta_1*X
# assumptions -> linear relationship btx Y and X;
#             -> Normality and randomness of the errors ~ N(0, sigma^2)
# parameters to estimate -> coefficients (beta), variability of the errors (sigma)
#We use the command lm that is for linear model
regression = lm(Weight ~ Cholesterol, data = males)
regression = lm(males$Weight ~ males$Cholesterol)
# Now let's draw the line on the scatterplot
plot(males$Cholesterol, males$Weight,
ylab = "Weight", xlab = "Cholesterol", pch = 19 )
abline( regression, col = "red", lwd = 2 )
males$Patient
# In R, through the summary () command, you get a lot of information on the built model
summary(regression)
# Specifically, the first column of the "Coefficients" table tells us the
# values of the estimated coefficients of the line;
# Then we have the standard error of the estimate, the test statistics and the last column tells us
# if the corresponding coefficient must be included in the
# regression (i.e. if the variable that we are using is significant in describing/predicting
# the outcome).
# H0: coeff = 0 vs H1 coeff different from 0 -> if the value of Pr (> | t |) is small (<0.05)
# then the coefficient must be considered non-zero, otherwise
# can be considered equal to 0 and therefore must be excluded from the model.
# COMMENTS: R ^ 2 tells us the percentage of variability of the data that
# our model can explain.
# Here, R ^ 2 is telling us that our model explains
# just under half the variability of our data (39%).
## ASSUMPTIONS ABOUT THE ERROR
# We assume that the errors are random variables i.i.d. as a
# Gaussian with mean 0 and variance sigma ^ 2.
# The variance is constant and does not depend
# on observations (it is the same for all observations: this is called the
#"homoskedasticity property").
# When doing a regression, therefore, you need first to check that
# the residuals are actually distributed like a normal.
# To do this you can use two graphs and a test:
# 1) view the residual histogram to get an idea of the shape
# of the residual distribution, which we expect to be symmetric
# 2) visualization of the QQ plot, which is a graph that represents
# the relationship between empirical residual quantiles and theoretical
# quantiles of a Gaussian random variable (-> If the points are arranged
# along a straight line, this indicates normality of the residuals).
# 3) Shapiro-wilk test (H0: residuals distributed as a normal)
names(regression)
regression$coefficients
res = regression$residuals
res
# STANDARDIZATION OF RESIDUALS
# This procedure allows you to analyze the behavior of residuals.
# Standardized residuals are obtained by dividing
# the residuals of the model by their estimated standard deviation,
# which in the summary is called Residual standard error.
# Calculating standardized residuals is convenient as
# allows comparison with the normal standard N (0,1).
# In other words, how much data is left out of range (which should contain 95%)
# is evaluated; in this way becomes directly [-2, 2] (red lines of the fourth graph)
res.std = res / summary(regression)$sigma
quartz()
par( mfrow = c( 2, 2 ) )
#normal distribution
# 1
hist(res, prob = TRUE, breaks = 10, col = "lightblue", main = "Residual histogram",
ylim = c( 0 , 0.05))
griglia = sort(res)
lines( griglia, dnorm( griglia, mean = 0, sd = sd( res ) ),
col = 'royalblue', lwd = 2 )
# 2
qqnorm(res, pch = 16)
qqline(res, col = "red", lwd = 2)
# homoskedasticity
# 3
plot(regression$fitted.values, res, pch = 16,
xlab = 'fitted y', ylab = 'residuals', main = 'Residuals' )
abline( h = 0, col = "grey", lwd = 2, lty = 2 )
# 4
plot( regression$fitted.values, res.std, pch = 16,
xlab = 'fitted y', ylab = 'standardized residuals', main = 'Standardized residuals' )
abline( h = 2, col = "red", lwd = 2, lty = 2 )
abline( h = 0, col = "grey", lwd = 2, lty = 2 )
abline( h = -2, col = "red", lwd = 2, lty = 2 )
shapiro.test(res) # we don't reject null hypothesis of normality
# COMMENTS: no particular asymmetries are seen from the histogram and
# the points in the QQ plot appear to be arranged along a straight line (with small
# deviations at the extremes), but the Shapiro Wilk test confirms that the residuals can be considered normal.
# Furthermore, from the residuals graph, we see a random cloud centered at 0 and
# the variance appears to be constant, therefore residuals can be considered homoskedastic.
# EXERCISE 2 --------------------------------------------------------------
# Analysis of the data 'anscombe' present in the package 'datasets' of R.
# 'anscombe' contains 4 x-y dataset which have the same mean, variance, correlation,
#            and regression line, yet are quite different.
# Study the relation between x_i and y_i.
library(datasets)
data(anscombe)
anscombe
# Sample size
n=dim(anscombe)[1]
n
# Compute means, variances, correlations
sample.means=apply(anscombe,2,mean)
sample.means
sample.var=apply(anscombe,2,var)
sample.var
cor=cor(anscombe[,1:4],anscombe[,5:8])
cor
### CASE 1 : predictor = x1, response = y1
X=anscombe$x1
Y=anscombe$y1
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 1', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[1]), ylab=expression(y[1]))
# Linear regression
result1=lm(Y ~ X)
summary(result1)
# Add regression line to the plot
coef1=result1$coef
abline(coef1[1],coef1[2],col='red',lwd=2)
#or
abline(result1, col = "red", lwd = 2 )
# Data seem well represented by the regression line
### CASE 2 : predictor = x2, response = y2
X=anscombe$x2
Y=anscombe$y2
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 2', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[2]), ylab=expression(y[2]))
# Linear regression
result2=lm(Y ~ X)
summary(result2)
# Add regression line to the plot
coef2=result2$coef
abline(coef2[1],coef2[2],col='red',lwd=2)
# Data don't seem well represented by the regression line...
# They seem to have a quadratic trend (parabola)
### CASE 3 : predictor = x3, response = y3
X=anscombe$x3
Y=anscombe$y3
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 3', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[3]), ylab=expression(y[3]))
# How to detect and remove the outlier?
which(Y > 10)
X[which(Y > 10)]
Y[which(Y > 10)]
X = X[-which(Y > 10)]
Y = Y[-which(Y > 10)]
#
# Linear regression
result3=lm(Y ~ X)
summary(result3)
# Add regression line to the plot
coef3=result3$coef
abline(coef3[1],coef3[2],col='red',lwd=2)
# It seems that the data lie on a straight line,
# but there is an outlier (leverage point)
### CASE 4 : predictor = x4, response = y4
X=anscombe$x4
Y=anscombe$y4
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 4', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[4]), ylab=expression(y[4]))
# Linear regression
result4=lm(Y ~ X)
summary(result4)
# Add regression line to the plot
coef4=result4$coef
abline(coef4[1],coef4[2],col='red',lwd=2)
# All the data are concentrated on the same X,
# except for one single observation with a different X
# (leverage point)
### How can I realize that there are problems in the regressions 2, 3 and 4?
### Use diagnostic plots!
# Get residuals
res1=result1$residuals
res2=result2$residuals
res3=result3$residuals
res4=result4$residuals
# Plot of residuals vs X, in the 4 cases
quartz()
par(mfrow=c(2,2))
plot(anscombe$x1,res1,main='Residuals vs X, case 1',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x2,res2,main='Residuals vs X, case 2',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x3,res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
plot(anscombe$x1,res1,main='Residuals vs X, case 1',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x2,res2,main='Residuals vs X, case 2',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x3,res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
res3
anscombe$x3
plot(anscombe$x4,res4,main='Residuals vs X, case 4',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
# Get fitted Y
Y_hat1=result1$fitted.values
result3$residuals
anscombe
res1
res3
result3
# How to detect and remove the outlier?
which(Y > 10)
X[which(Y > 10)]
Y[which(Y > 10)]
X = X[-which(Y > 10)]
Y = Y[-which(Y > 10)]
X
Y
### CASE 3 : predictor = x3, response = y3
X=anscombe$x3
Y=anscombe$y3
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 3', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[3]), ylab=expression(y[3]))
# How to detect and remove the outlier?
which(Y > 10)
X[which(Y > 10)]
Y[which(Y > 10)]
X = X[-which(Y > 10)]
Y = Y[-which(Y > 10)]
X
Y
# Linear regression
result3=lm(Y ~ X)
summary(result3)
# Add regression line to the plot
coef3=result3$coef
abline(coef3[1],coef3[2],col='red',lwd=2)
res3=result3$residuals
plot(anscombe$x3,res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
anscombe$x3
anscombe
result3
result3$residuals
result3
result2$residuals
result1$residuals
result4$residuals
result3$model
### CASE 3 : predictor = x3, response = y3
X=anscombe$x3
Y=anscombe$y3
# Scatterplot of Y vs X
quartz()
plot(X,Y,main='Scatterplot of Y vs X, case 3', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[3]), ylab=expression(y[3]))
# How to detect and remove the outlier?
which(Y > 10)
X[which(Y > 10)]
Y[which(Y > 10)]
X = X[-which(Y > 10)]
Y = Y[-which(Y > 10)]
X
# Linear regression
result3=lm(Y ~ X)
summary(result3)
result3$model
result3$residuals
plot(anscombe$x3[-which(anscombe$x3 > 10)],res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
anscombe$x3[-which(anscombe$x3 > 10)]
anscombe$x3
anscombe$x3
res3
res3
plot(anscombe$x3,res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
anscombe$x3
plot(anscombe$x3[-which(anscombe$y3 > 10)],res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x3[-which(anscombe$y3 > 10)],res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x4,res4,main='Residuals vs X, case 4',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x1,res1,main='Residuals vs X, case 1',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x2,res2,main='Residuals vs X, case 2',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x3[-which(anscombe$y3 > 10)],res3,main='Residuals vs X, case 3',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(anscombe$x4,res4,main='Residuals vs X, case 4',lwd=2,
xlab='X',ylab='Residuals')
abline(h=0,lwd=2,col='red')
# Get fitted Y
Y_hat1=result1$fitted.values
Y_hat2=result2$fitted.values
Y_hat3=result3$fitted.values
Y_hat4=result4$fitted.values
# Plot of residuals vs fitted Y, in the 4 cases
quartz()
par(mfrow=c(2,2))
plot(Y_hat1,res1,main='Residuals vs fitted Y, case 1',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(Y_hat2,res2,main='Residuals vs fitted Y, case 2',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(Y_hat3,res3,main='Residuals vs fitted Y, case 3',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
plot(Y_hat4,res4,main='Residuals vs fitted Y, case 4',lwd=2,
xlab='Fitted Y',ylab='Residuals')
abline(h=0,lwd=2,col='red')
# QQ-plot of residuals, in the 4 cases
quartz()
par(mfrow=c(2,2))
qqnorm(res1,main='QQ-plot of residuals, case 1',lwd=2)
qqline(res1,lwd=2,col='red')
qqnorm(res2,main='QQ-plot of residuals, case 2',lwd=2)
qqline(res2,lwd=2,col='red')
qqnorm(res3,main='QQ-plot of residuals, case 3',lwd=2)
qqline(res3,lwd=2,col='red')
qqnorm(res4,main='QQ-plot of residuals, case 4',lwd=2)
qqline(res4,lwd=2,col='red')
# Shapiro-Wilk test for the normality of residuals
shapiro.test(res1)
# Shapiro-Wilk test for the normality of residuals
shapiro.test(res1)
shapiro.test(res2)
shapiro.test(res3)
shapiro.test(res4)
### CASE 2 : predictor = x2, response = y2
### Add a quadratic term!
X=anscombe$x2
Y=anscombe$y2
# Scatterplot of Y vs X
quarz()
plot(X,Y,main='Scatterplot of Y vs X, case 2', lwd=2,xlim=c(4,19),ylim=c(3,13),
xlab=expression(x[2]), ylab=expression(y[2]))
devtools::load_all(".")
